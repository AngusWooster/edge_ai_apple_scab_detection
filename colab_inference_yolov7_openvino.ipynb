{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Mount google drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15067,"status":"ok","timestamp":1685322434811,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"},"user_tz":-480},"id":"950FXE1V0ZJ_","outputId":"046bf0ef-4885-4643-ff64-c2f5bc561323"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","drive  sample_data\n","'Colab Notebooks'\n"," How_CNNs_work.pdf\n","'How neural networks work.pdf'\n"," interview\n","'Introducing-Bluetooth-LE-Audio-book - Copy.pdf'\n"," machine_leanring_course\n"," MyDrive\n"," ntust_course\n","'ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer.md'\n"," windows_linux_share\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')\n","!ln -s /content/drive/MyDrive/ /my_drive\n","!ls\n","!ls /my_drive"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Download Apple Scab Detection from github"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1959,"status":"ok","timestamp":1685322439625,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"},"user_tz":-480},"id":"UbRSB8Kk0iVF","outputId":"dd00b7e5-eaff-463f-f8df-5c42eaacae69"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'edge_ai_apple_scab_detection'...\n","remote: Enumerating objects: 142, done.\u001b[K\n","remote: Counting objects: 100% (96/96), done.\u001b[K\n","remote: Compressing objects: 100% (89/89), done.\u001b[K\n","remote: Total 142 (delta 5), reused 95 (delta 4), pack-reused 46\u001b[K\n","Receiving objects: 100% (142/142), 51.41 MiB | 48.39 MiB/s, done.\n","Resolving deltas: 100% (5/5), done.\n","/content/edge_ai_apple_scab_detection\n"]}],"source":["# download Apple Scab Detection\n","!git clone https://github.com/AngusWooster/edge_ai_apple_scab_detection.git\n","%cd edge_ai_apple_scab_detection/"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Install requirement packages"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":24256,"status":"ok","timestamp":1685322882171,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"},"user_tz":-480},"id":"v8JtLH80_pA-","outputId":"3bfbe57d-351e-4fc4-dbe1-09cd541d947b"},"outputs":[],"source":["# Install requirement package\n","!pip install openvino\n","!pip install openvino-dev\n","!pip install gdown\n","!pip install onnx"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Download pre-trained weights"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6033,"status":"ok","timestamp":1685322595278,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"},"user_tz":-480},"id":"LC9KQSAC-q_F","outputId":"f20840df-bdba-479d-87a7-65f2072fdaf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/edge_ai_apple_scab_detection/faster_rcnn/outputs/training/final_dataset\n","Downloading...\n","From: https://drive.google.com/uc?id=1S6LizlqZSI8zf4Blsq8gmwFHtPp81U5_\n","To: /content/edge_ai_apple_scab_detection/faster_rcnn/outputs/training/final_dataset/best_model.pth\n","100% 173M/173M [00:03<00:00, 55.0MB/s]\n","/content/edge_ai_apple_scab_detection\n","/content/edge_ai_apple_scab_detection/yolov7/weight/final_dataset\n","Downloading...\n","From: https://drive.google.com/uc?id=1amTSXUGkdwDn96tr62obRfftmVfmxI_u\n","To: /content/edge_ai_apple_scab_detection/yolov7/weight/final_dataset/best.pt\n","100% 74.8M/74.8M [00:00<00:00, 87.0MB/s]\n","/content/edge_ai_apple_scab_detection/yolov7\n"]}],"source":["!mkdir -p faster_rcnn/outputs/training/final_dataset\n","!mkdir -p yolov7/weight/final_dataset\n","\n","%cd faster_rcnn/outputs/training/final_dataset\n","# Download faster_rcnn pre-trained weight\n","!gdown https://drive.google.com/uc?id=1S6LizlqZSI8zf4Blsq8gmwFHtPp81U5_\n","\n","\n","%cd ../../../../\n","%cd yolov7/weight/final_dataset/\n","# Download yolov7 pre-trained weight\n","!gdown https://drive.google.com/uc?id=1amTSXUGkdwDn96tr62obRfftmVfmxI_u\n","\n","%cd ../../"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ha5n5J-ZDr23"},"source":["# Export model to onnx "]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28903,"status":"ok","timestamp":1685322794028,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"},"user_tz":-480},"id":"unZY2kBV0QQ-","outputId":"1f319234-d832-4eed-b890-e4e3c50b8ac0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Import onnx_graphsurgeon failure: No module named 'onnx_graphsurgeon'\n","Namespace(weights='weight/final_dataset/best.pt', img_size=[640, 640], batch_size=1, dynamic=False, dynamic_batch=False, grid=True, end2end=False, max_wh=None, topk_all=100, iou_thres=0.45, conf_thres=0.25, device='cpu', simplify=False, include_nms=False, fp16=False, int8=False)\n","YOLOR ðŸš€ 774d1e9 torch 2.0.1+cu118 CPU\n","\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n","Model Summary: 314 layers, 36481772 parameters, 6194944 gradients\n","\n","Starting TorchScript export with torch 2.0.1+cu118...\n","TorchScript export success, saved as weight/final_dataset/best.torchscript.pt\n","CoreML export failure: No module named 'coremltools'\n","\n","Starting TorchScript-Lite export with torch 2.0.1+cu118...\n","TorchScript-Lite export success, saved as weight/final_dataset/best.torchscript.ptl\n","\n","Starting ONNX export with onnx 1.14.0...\n","============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n","verbose: False, log level: Level.ERROR\n","======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n","\n","ONNX export success, saved as weight/final_dataset/best.onnx\n","\n","Export complete (24.80s). Visualize with https://github.com/lutzroeder/netron.\n"]}],"source":["# !python3 export.py --help\n","!python3 -W ignore export.py --weights weight/final_dataset/best.pt --grid"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1685322887412,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"},"user_tz":-480},"id":"e9Js9BxJ2Ya6"},"outputs":[],"source":["import sys\n","import time\n","from pathlib import Path\n","from typing import List, Tuple, Dict\n","import os\n","import glob\n","import cv2\n","import matplotlib.pyplot as plt\n","from utils.general import scale_coords, non_max_suppression\n","from openvino.runtime import Model\n","from openvino.tools import mo\n","from openvino.runtime import Core\n","\n","import numpy as np\n","import torch\n","from tqdm import tqdm\n","from PIL import Image\n","from utils.datasets import letterbox\n","from utils.plots import plot_one_box"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# OpenVINO model convert"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":5445,"status":"ok","timestamp":1685322900154,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"},"user_tz":-480},"id":"3lu4G3RH2a_0"},"outputs":[],"source":["model = mo.convert_model('weight/final_dataset/best.onnx')\n","core = Core()\n","# load model on CPU device\n","compiled_model = core.compile_model(model, 'CPU')"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":401,"status":"ok","timestamp":1685322916284,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"},"user_tz":-480},"id":"qW6I-91m2eaT"},"outputs":[],"source":["def preprocess_image(img0: np.ndarray):\n","    \"\"\"\n","    Preprocess image according to YOLOv7 input requirements. \n","    Takes image in np.array format, resizes it to specific size using letterbox resize, converts color space from BGR (default in OpenCV) to RGB and changes data layout from HWC to CHW.\n","    \n","    Parameters:\n","      img0 (np.ndarray): image for preprocessing\n","    Returns:\n","      img (np.ndarray): image after preprocessing\n","      img0 (np.ndarray): original image\n","    \"\"\"\n","    # resize\n","    img = letterbox(img0, auto=False)[0]\n","    \n","    # Convert\n","    img = img.transpose(2, 0, 1)\n","    img = np.ascontiguousarray(img)\n","    return img, img0\n","\n","\n","def prepare_input_tensor(image: np.ndarray):\n","    \"\"\"\n","    Converts preprocessed image to tensor format according to YOLOv7 input requirements. \n","    Takes image in np.array format with unit8 data in [0, 255] range and converts it to torch.Tensor object with float data in [0, 1] range\n","    \n","    Parameters:\n","      image (np.ndarray): image for conversion to tensor\n","    Returns:\n","      input_tensor (torch.Tensor): float tensor ready to use for YOLOv7 inference\n","    \"\"\"\n","    input_tensor = image.astype(np.float32)  # uint8 to fp16/32\n","    input_tensor /= 255.0  # 0 - 255 to 0.0 - 1.0\n","    \n","    if input_tensor.ndim == 3:\n","        input_tensor = np.expand_dims(input_tensor, 0)\n","    return input_tensor\n","\n","\n","# label names for visualization\n","NAMES = ['scab']\n","\n","# colors for visualization\n","# COLORS = {name: [np.random.randint(0, 255) for _ in range(3)]\n","#           for i, name in enumerate(NAMES)}\n","COLORS = {'scab': [255, 0, 0]}"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":425,"status":"ok","timestamp":1685322920174,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"},"user_tz":-480},"id":"g6Obi0oi2fb8"},"outputs":[],"source":["def detect(model: Model, image_path: Path, conf_thres: float = 0.25, iou_thres: float = 0.45, classes: List[int] = None, agnostic_nms: bool = False):\n","    \"\"\"\n","    OpenVINO YOLOv7 model inference function. Reads image, preprocess it, runs model inference and postprocess results using NMS.\n","    Parameters:\n","        model (Model): OpenVINO compiled model.\n","        image_path (Path): input image path.\n","        conf_thres (float, *optional*, 0.25): minimal accpeted confidence for object filtering\n","        iou_thres (float, *optional*, 0.45): minimal overlap score for remloving objects duplicates in NMS\n","        classes (List[int], *optional*, None): labels for prediction filtering, if not provided all predicted labels will be used\n","        agnostic_nms (bool, *optiona*, False): apply class agnostinc NMS approach or not\n","    Returns:\n","       pred (List): list of detections with (n,6) shape, where n - number of detected boxes in format [x1, y1, x2, y2, score, label] \n","       orig_img (np.ndarray): image before preprocessing, can be used for results visualization\n","       inpjut_shape (Tuple[int]): shape of model input tensor, can be used for output rescaling\n","    \"\"\"\n","    output_blob = model.output(0)\n","    img = np.array(Image.open(image_path))\n","    preprocessed_img, orig_img = preprocess_image(img)\n","    input_tensor = prepare_input_tensor(preprocessed_img)\n","    t1 = time.time()\n","    predictions = torch.from_numpy(model(input_tensor)[output_blob])\n","    model_inference_time = time.time() - t1\n","    pred = non_max_suppression(predictions, conf_thres, iou_thres, classes=classes, agnostic=agnostic_nms)\n","    return pred, orig_img, input_tensor.shape, model_inference_time\n","\n","\n","def draw_boxes(predictions: np.ndarray, input_shape: Tuple[int], image: np.ndarray, names: List[str], colors: Dict[str, int]):\n","    \"\"\"\n","    Utility function for drawing predicted bounding boxes on image\n","    Parameters:\n","        predictions (np.ndarray): list of detections with (n,6) shape, where n - number of detected boxes in format [x1, y1, x2, y2, score, label]\n","        image (np.ndarray): image for boxes visualization\n","        names (List[str]): list of names for each class in dataset\n","        colors (Dict[str, int]): mapping between class name and drawing color\n","    Returns:\n","        image (np.ndarray): box visualization result\n","    \"\"\"\n","    if not len(predictions):\n","        return image\n","    # Rescale boxes from input size to original image size\n","    predictions[:, :4] = scale_coords(input_shape[2:], predictions[:, :4], image.shape).round()\n","\n","    # Write results\n","    for *xyxy, conf, cls in reversed(predictions):\n","        label = f'{names[int(cls)]} {conf:.2f}'\n","        plot_one_box(xyxy, image, label=label, color=colors[names[int(cls)]], line_thickness=3)\n","    return image\n","\n","def openvino_inference_dir_create(dir_name = \"eval\") -> str:\n","    \"\"\"\n","    This functions counts the number of evaluation directories already present\n","    and creates a new one in `outputs/evaluation/`.\n","    And returns the directory path.\n","    \"\"\"\n","    dir_path = 'outputs/openvino_inference' + '/' + dir_name\n","    if not os.path.exists(dir_path):\n","        os.makedirs(dir_path)\n","    num_eval_dirs_present = len(os.listdir(dir_path))\n","    next_dir_num = num_eval_dirs_present + 1\n","    new_dir_path = f\"{dir_path}/res_{next_dir_num}\"\n","    os.makedirs(new_dir_path, exist_ok=True)\n","    return new_dir_path"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74146,"status":"ok","timestamp":1685323001599,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"},"user_tz":-480},"id":"ilk47bNC2jnd","outputId":"9ef63f99-0423-47ac-84b1-eac0b2efa9c4"},"outputs":[],"source":["%matplotlib inline\n","validation_image_path = \"../dataset/final_dataset/images/valid/\"\n","image_file_types = ['*.jpg', '*.jpeg', '*.JPG']\n","all_image_paths = []\n","\n","output_dir = openvino_inference_dir_create()\n","print(f\"inference output: {output_dir}\")\n","\n","for file_type in image_file_types:\n","    all_image_paths.extend(sorted(glob.glob(os.path.join(validation_image_path, file_type))))\n","\n","for i, img_path in enumerate(tqdm(all_image_paths)):\n","    boxes, image, input_shape, model_inference_time = detect(compiled_model, img_path)\n","    image_with_boxes = draw_boxes(boxes[0], input_shape, image, NAMES, COLORS)\n","    if i == 0:\n","        plt.imshow(image_with_boxes)\n","        plt.show()\n","    # save image\n","    image = cv2.cvtColor(image_with_boxes, cv2.COLOR_RGB2BGR)\n","    cv2.imwrite(f\"{output_dir}/image_{i}.jpg\", image)\n","    print(f\"{i:03d}: model_inference = ({(1E3 * (model_inference_time)):.1f}ms)\")\n","print(f\"done\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOyXfP+frUE6DBYG0V11Mk3","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
