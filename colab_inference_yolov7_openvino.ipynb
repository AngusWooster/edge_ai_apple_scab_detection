{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOyXfP+frUE6DBYG0V11Mk3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')\n","!ln -s /content/drive/MyDrive/ /my_drive\n","!ls\n","!ls /my_drive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"950FXE1V0ZJ_","executionInfo":{"status":"ok","timestamp":1685322434811,"user_tz":-480,"elapsed":15067,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"}},"outputId":"046bf0ef-4885-4643-ff64-c2f5bc561323"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","drive  sample_data\n","'Colab Notebooks'\n"," How_CNNs_work.pdf\n","'How neural networks work.pdf'\n"," interview\n","'Introducing-Bluetooth-LE-Audio-book - Copy.pdf'\n"," machine_leanring_course\n"," MyDrive\n"," ntust_course\n","'ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer.md'\n"," windows_linux_share\n"]}]},{"cell_type":"code","source":["# download Apple Scab Detection\n","!git clone https://github.com/AngusWooster/edge_ai_apple_scab_detection.git\n","%cd edge_ai_apple_scab_detection/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UbRSB8Kk0iVF","executionInfo":{"status":"ok","timestamp":1685322439625,"user_tz":-480,"elapsed":1959,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"}},"outputId":"dd00b7e5-eaff-463f-f8df-5c42eaacae69"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'edge_ai_apple_scab_detection'...\n","remote: Enumerating objects: 142, done.\u001b[K\n","remote: Counting objects: 100% (96/96), done.\u001b[K\n","remote: Compressing objects: 100% (89/89), done.\u001b[K\n","remote: Total 142 (delta 5), reused 95 (delta 4), pack-reused 46\u001b[K\n","Receiving objects: 100% (142/142), 51.41 MiB | 48.39 MiB/s, done.\n","Resolving deltas: 100% (5/5), done.\n","/content/edge_ai_apple_scab_detection\n"]}]},{"cell_type":"code","source":["# Install requirement package\n","!pip install openvino\n","!pip install openvino-dev\n","!pip install gdown\n","!pip install onnx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"v8JtLH80_pA-","executionInfo":{"status":"ok","timestamp":1685322882171,"user_tz":-480,"elapsed":24256,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"}},"outputId":"3bfbe57d-351e-4fc4-dbe1-09cd541d947b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: openvino in /usr/local/lib/python3.10/dist-packages (2022.3.0)\n","Requirement already satisfied: numpy<=1.23.4,>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from openvino) (1.22.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting openvino-dev\n","  Downloading openvino_dev-2022.3.0-9052-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting addict>=2.4.0 (from openvino-dev)\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (0.7.1)\n","Collecting jstyleson>=0.0.2 (from openvino-dev)\n","  Downloading jstyleson-0.0.2.tar.gz (2.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting networkx<=2.8.8 (from openvino-dev)\n","  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<=1.23.4,>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (1.22.4)\n","Requirement already satisfied: opencv-python>=4.5 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (4.7.0.72)\n","Collecting openvino-telemetry>=2022.1.0 (from openvino-dev)\n","  Downloading openvino_telemetry-2022.3.0-py3-none-any.whl (20 kB)\n","Collecting pandas~=1.3.5 (from openvino-dev)\n","  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow>=8.1.2 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (8.4.0)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (6.0)\n","Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (2.27.1)\n","Collecting texttable>=1.6.3 (from openvino-dev)\n","  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (4.65.0)\n","Requirement already satisfied: openvino==2022.3.0 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (2022.3.0)\n","Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (1.10.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.3.5->openvino-dev) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.3.5->openvino-dev) (2022.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->openvino-dev) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->openvino-dev) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->openvino-dev) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->openvino-dev) (3.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.3->pandas~=1.3.5->openvino-dev) (1.16.0)\n","Building wheels for collected packages: jstyleson\n","  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jstyleson: filename=jstyleson-0.0.2-py3-none-any.whl size=2384 sha256=31d3f6f16fdac07bafabd1e92cd2f4a80f85e8a6fb3138e235459d7dfd18cf30\n","  Stored in directory: /root/.cache/pip/wheels/12/51/c6/a1e751db88203e11c6d9ffe4683ca3d8c14b1479639bec1006\n","Successfully built jstyleson\n","Installing collected packages: texttable, jstyleson, addict, networkx, pandas, openvino-telemetry, openvino-dev\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 3.1\n","    Uninstalling networkx-3.1:\n","      Successfully uninstalled networkx-3.1\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.5.3\n","    Uninstalling pandas-1.5.3:\n","      Successfully uninstalled pandas-1.5.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed addict-2.4.0 jstyleson-0.0.2 networkx-2.8.8 openvino-dev-2022.3.0 openvino-telemetry-2022.3.0 pandas-1.3.5 texttable-1.6.7\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["openvino","pandas"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.14.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.22.4)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.5.0)\n"]}]},{"cell_type":"code","source":["!mkdir -p faster_rcnn/outputs/training/final_dataset\n","!mkdir -p yolov7/weight/final_dataset"],"metadata":{"id":"N0kfuv27DIVq","executionInfo":{"status":"ok","timestamp":1685322577641,"user_tz":-480,"elapsed":474,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["%cd faster_rcnn/outputs/training/final_dataset\n","# Download faster_rcnn pre-trained weight\n","!gdown https://drive.google.com/uc?id=1S6LizlqZSI8zf4Blsq8gmwFHtPp81U5_\n","\n","\n","%cd ../../../../\n","%cd yolov7/weight/final_dataset/\n","# Download yolov7 pre-trained weight\n","!gdown https://drive.google.com/uc?id=1amTSXUGkdwDn96tr62obRfftmVfmxI_u\n","\n","%cd ../../"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LC9KQSAC-q_F","executionInfo":{"status":"ok","timestamp":1685322595278,"user_tz":-480,"elapsed":6033,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"}},"outputId":"f20840df-bdba-479d-87a7-65f2072fdaf1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/edge_ai_apple_scab_detection/faster_rcnn/outputs/training/final_dataset\n","Downloading...\n","From: https://drive.google.com/uc?id=1S6LizlqZSI8zf4Blsq8gmwFHtPp81U5_\n","To: /content/edge_ai_apple_scab_detection/faster_rcnn/outputs/training/final_dataset/best_model.pth\n","100% 173M/173M [00:03<00:00, 55.0MB/s]\n","/content/edge_ai_apple_scab_detection\n","/content/edge_ai_apple_scab_detection/yolov7/weight/final_dataset\n","Downloading...\n","From: https://drive.google.com/uc?id=1amTSXUGkdwDn96tr62obRfftmVfmxI_u\n","To: /content/edge_ai_apple_scab_detection/yolov7/weight/final_dataset/best.pt\n","100% 74.8M/74.8M [00:00<00:00, 87.0MB/s]\n","/content/edge_ai_apple_scab_detection/yolov7\n"]}]},{"cell_type":"markdown","source":["# Export model to onnx "],"metadata":{"id":"Ha5n5J-ZDr23"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"unZY2kBV0QQ-","executionInfo":{"status":"ok","timestamp":1685322794028,"user_tz":-480,"elapsed":28903,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1f319234-d832-4eed-b890-e4e3c50b8ac0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Import onnx_graphsurgeon failure: No module named 'onnx_graphsurgeon'\n","Namespace(weights='weight/final_dataset/best.pt', img_size=[640, 640], batch_size=1, dynamic=False, dynamic_batch=False, grid=True, end2end=False, max_wh=None, topk_all=100, iou_thres=0.45, conf_thres=0.25, device='cpu', simplify=False, include_nms=False, fp16=False, int8=False)\n","YOLOR ðŸš€ 774d1e9 torch 2.0.1+cu118 CPU\n","\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n","Model Summary: 314 layers, 36481772 parameters, 6194944 gradients\n","\n","Starting TorchScript export with torch 2.0.1+cu118...\n","TorchScript export success, saved as weight/final_dataset/best.torchscript.pt\n","CoreML export failure: No module named 'coremltools'\n","\n","Starting TorchScript-Lite export with torch 2.0.1+cu118...\n","TorchScript-Lite export success, saved as weight/final_dataset/best.torchscript.ptl\n","\n","Starting ONNX export with onnx 1.14.0...\n","============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n","verbose: False, log level: Level.ERROR\n","======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n","\n","ONNX export success, saved as weight/final_dataset/best.onnx\n","\n","Export complete (24.80s). Visualize with https://github.com/lutzroeder/netron.\n"]}],"source":["# !python3 export.py --help\n","!python3 -W ignore export.py --weights weight/final_dataset/best.pt --grid"]},{"cell_type":"code","source":["import sys\n","import time\n","from pathlib import Path\n","from typing import List, Tuple, Dict\n","import os\n","import glob\n","import cv2\n","import matplotlib.pyplot as plt\n","from utils.general import scale_coords, non_max_suppression\n","from openvino.runtime import Model\n","from openvino.tools import mo\n","from openvino.runtime import Core"],"metadata":{"id":"e9Js9BxJ2Ya6","executionInfo":{"status":"ok","timestamp":1685322887412,"user_tz":-480,"elapsed":3,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QZH0d0mu2Z97"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = mo.convert_model('weight/final_dataset/best.onnx')\n","core = Core()\n","# load model on CPU device\n","compiled_model = core.compile_model(model, 'CPU')"],"metadata":{"id":"3lu4G3RH2a_0","executionInfo":{"status":"ok","timestamp":1685322900154,"user_tz":-480,"elapsed":5445,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from PIL import Image\n","from utils.datasets import letterbox\n","from utils.plots import plot_one_box\n","\n","\n","def preprocess_image(img0: np.ndarray):\n","    \"\"\"\n","    Preprocess image according to YOLOv7 input requirements. \n","    Takes image in np.array format, resizes it to specific size using letterbox resize, converts color space from BGR (default in OpenCV) to RGB and changes data layout from HWC to CHW.\n","    \n","    Parameters:\n","      img0 (np.ndarray): image for preprocessing\n","    Returns:\n","      img (np.ndarray): image after preprocessing\n","      img0 (np.ndarray): original image\n","    \"\"\"\n","    # resize\n","    img = letterbox(img0, auto=False)[0]\n","    \n","    # Convert\n","    img = img.transpose(2, 0, 1)\n","    img = np.ascontiguousarray(img)\n","    return img, img0\n","\n","\n","def prepare_input_tensor(image: np.ndarray):\n","    \"\"\"\n","    Converts preprocessed image to tensor format according to YOLOv7 input requirements. \n","    Takes image in np.array format with unit8 data in [0, 255] range and converts it to torch.Tensor object with float data in [0, 1] range\n","    \n","    Parameters:\n","      image (np.ndarray): image for conversion to tensor\n","    Returns:\n","      input_tensor (torch.Tensor): float tensor ready to use for YOLOv7 inference\n","    \"\"\"\n","    input_tensor = image.astype(np.float32)  # uint8 to fp16/32\n","    input_tensor /= 255.0  # 0 - 255 to 0.0 - 1.0\n","    \n","    if input_tensor.ndim == 3:\n","        input_tensor = np.expand_dims(input_tensor, 0)\n","    return input_tensor\n","\n","\n","# label names for visualization\n","NAMES = ['scab']\n","\n","# colors for visualization\n","# COLORS = {name: [np.random.randint(0, 255) for _ in range(3)]\n","#           for i, name in enumerate(NAMES)}\n","COLORS = {'scab': [255, 0, 0]}"],"metadata":{"id":"qW6I-91m2eaT","executionInfo":{"status":"ok","timestamp":1685322916284,"user_tz":-480,"elapsed":401,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def detect(model: Model, image_path: Path, conf_thres: float = 0.25, iou_thres: float = 0.45, classes: List[int] = None, agnostic_nms: bool = False):\n","    \"\"\"\n","    OpenVINO YOLOv7 model inference function. Reads image, preprocess it, runs model inference and postprocess results using NMS.\n","    Parameters:\n","        model (Model): OpenVINO compiled model.\n","        image_path (Path): input image path.\n","        conf_thres (float, *optional*, 0.25): minimal accpeted confidence for object filtering\n","        iou_thres (float, *optional*, 0.45): minimal overlap score for remloving objects duplicates in NMS\n","        classes (List[int], *optional*, None): labels for prediction filtering, if not provided all predicted labels will be used\n","        agnostic_nms (bool, *optiona*, False): apply class agnostinc NMS approach or not\n","    Returns:\n","       pred (List): list of detections with (n,6) shape, where n - number of detected boxes in format [x1, y1, x2, y2, score, label] \n","       orig_img (np.ndarray): image before preprocessing, can be used for results visualization\n","       inpjut_shape (Tuple[int]): shape of model input tensor, can be used for output rescaling\n","    \"\"\"\n","    output_blob = model.output(0)\n","    img = np.array(Image.open(image_path))\n","    preprocessed_img, orig_img = preprocess_image(img)\n","    input_tensor = prepare_input_tensor(preprocessed_img)\n","    t1 = time.time()\n","    predictions = torch.from_numpy(model(input_tensor)[output_blob])\n","    model_inference_time = time.time() - t1\n","    pred = non_max_suppression(predictions, conf_thres, iou_thres, classes=classes, agnostic=agnostic_nms)\n","    return pred, orig_img, input_tensor.shape, model_inference_time\n","\n","\n","def draw_boxes(predictions: np.ndarray, input_shape: Tuple[int], image: np.ndarray, names: List[str], colors: Dict[str, int]):\n","    \"\"\"\n","    Utility function for drawing predicted bounding boxes on image\n","    Parameters:\n","        predictions (np.ndarray): list of detections with (n,6) shape, where n - number of detected boxes in format [x1, y1, x2, y2, score, label]\n","        image (np.ndarray): image for boxes visualization\n","        names (List[str]): list of names for each class in dataset\n","        colors (Dict[str, int]): mapping between class name and drawing color\n","    Returns:\n","        image (np.ndarray): box visualization result\n","    \"\"\"\n","    if not len(predictions):\n","        return image\n","    # Rescale boxes from input size to original image size\n","    predictions[:, :4] = scale_coords(input_shape[2:], predictions[:, :4], image.shape).round()\n","\n","    # Write results\n","    for *xyxy, conf, cls in reversed(predictions):\n","        label = f'{names[int(cls)]} {conf:.2f}'\n","        plot_one_box(xyxy, image, label=label, color=colors[names[int(cls)]], line_thickness=3)\n","    return image"],"metadata":{"id":"g6Obi0oi2fb8","executionInfo":{"status":"ok","timestamp":1685322920174,"user_tz":-480,"elapsed":425,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def openvino_inference_dir_create(dir_name = \"eval\") -> str:\n","    \"\"\"\n","    This functions counts the number of evaluation directories already present\n","    and creates a new one in `outputs/evaluation/`.\n","    And returns the directory path.\n","    \"\"\"\n","    dir_path = 'outputs/openvino_inference' + '/' + dir_name\n","    if not os.path.exists(dir_path):\n","        os.makedirs(dir_path)\n","    num_eval_dirs_present = len(os.listdir(dir_path))\n","    next_dir_num = num_eval_dirs_present + 1\n","    new_dir_path = f\"{dir_path}/res_{next_dir_num}\"\n","    os.makedirs(new_dir_path, exist_ok=True)\n","    return new_dir_path"],"metadata":{"id":"FNd5P1AR2h88","executionInfo":{"status":"ok","timestamp":1685322922331,"user_tz":-480,"elapsed":314,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["%matplotlib inline\n","validation_image_path = \"../dataset/final_dataset/images/valid/\"\n","image_file_types = ['*.jpg', '*.jpeg', '*.JPG']\n","all_image_paths = []\n","\n","output_dir = openvino_inference_dir_create()\n","print(f\"inference output: {output_dir}\")\n","\n","for file_type in image_file_types:\n","    all_image_paths.extend(sorted(glob.glob(os.path.join(validation_image_path, file_type))))\n","\n","for i, img_path in enumerate(all_image_paths):\n","    boxes, image, input_shape, model_inference_time = detect(compiled_model, img_path)\n","    image_with_boxes = draw_boxes(boxes[0], input_shape, image, NAMES, COLORS)\n","    if i == 0:\n","        plt.imshow(image_with_boxes)\n","        plt.show()\n","    # save image\n","    image = cv2.cvtColor(image_with_boxes, cv2.COLOR_RGB2BGR)\n","    cv2.imwrite(f\"{output_dir}/image_{i}.jpg\", image)\n","    print(f\"{i:03d}: model_inference = ({(1E3 * (model_inference_time)):.1f}ms)\")\n","print(f\"done\")"],"metadata":{"id":"ilk47bNC2jnd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685323001599,"user_tz":-480,"elapsed":74146,"user":{"displayName":"Angus Ku","userId":"10061947383962343241"}},"outputId":"9ef63f99-0423-47ac-84b1-eac0b2efa9c4"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["000: model_inference = (1593.6ms)\n","001: model_inference = (1549.3ms)\n","002: model_inference = (1227.3ms)\n","003: model_inference = (1196.6ms)\n","004: model_inference = (1192.0ms)\n","005: model_inference = (1198.5ms)\n","006: model_inference = (1199.2ms)\n","007: model_inference = (1192.5ms)\n","008: model_inference = (1209.8ms)\n","009: model_inference = (1197.2ms)\n","010: model_inference = (1395.0ms)\n","011: model_inference = (1494.9ms)\n","012: model_inference = (1522.8ms)\n","013: model_inference = (1639.6ms)\n","014: model_inference = (1209.8ms)\n","015: model_inference = (1207.0ms)\n","016: model_inference = (1198.7ms)\n","017: model_inference = (1205.9ms)\n","018: model_inference = (2047.9ms)\n","019: model_inference = (2589.9ms)\n","020: model_inference = (3979.3ms)\n","021: model_inference = (2023.1ms)\n","022: model_inference = (1212.2ms)\n","023: model_inference = (1214.5ms)\n","024: model_inference = (1195.9ms)\n","025: model_inference = (1191.6ms)\n","026: model_inference = (1384.2ms)\n","027: model_inference = (1492.5ms)\n","028: model_inference = (1526.0ms)\n","029: model_inference = (1234.4ms)\n","030: model_inference = (1202.5ms)\n","031: model_inference = (1196.9ms)\n","032: model_inference = (1196.6ms)\n","033: model_inference = (1191.9ms)\n","034: model_inference = (1201.7ms)\n","035: model_inference = (1399.9ms)\n","036: model_inference = (1489.0ms)\n","037: model_inference = (1527.0ms)\n","038: model_inference = (1231.5ms)\n","039: model_inference = (1198.7ms)\n","done\n"]}]}]}